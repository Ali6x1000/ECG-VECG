import os
import pandas as pd
import numpy as np
import cv2
import pickle
import keras
import tensorflow as tf
import matplotlib
matplotlib.use('Agg') # IMPORTANT: Use 'Agg' backend for headless Slurm jobs
import matplotlib.pyplot as plt
# Removed shutil and google.colab
from keras import mixed_precision

policy = mixed_precision.Policy('mixed_float16')
mixed_precision.set_global_policy(policy)

# ======================================================
# 0. Configuration
# ======================================================

# --- ‚ùóÔ∏è CHOOSE YOUR EXPERIMENT ---
# Set this to 'ecg_only', 'vcg_only', or 'combined'
TRAINING_MODE = 'ecg_only'
# ---

VAL_FOLD = 9
TEST_FOLD = 10
IMG_SIZE = (224, 224)
BATCH_SIZE = 32
DTYPE = np.float16

# --- File Paths (Simplified for HPC/Local) ---
# Assumes all files are in the same directory as this script

META_CSV_PATH = "ptbxl_database.csv"

# --- Source Pickle File ---
# This is the .pkl file generated by your other script
SOURCE_PICKLE_PATH = "ecg_vcg_image_cache_hr_1x224x224.pkl"

# --- New Mode-Specific Cache Paths ---
# These will be generated based on the TRAINING_MODE
IMAGE_CACHE_PATH = f"ptbxl_{TRAINING_MODE}_cache.npy"
STATS_PATH = f"ptbxl_{TRAINING_MODE}_stats.npz"

# --- Model and Plot Paths ---
BEST_MODEL_PATH = f"ecg_resnet50_{TRAINING_MODE}.keras"
PLOT_MAE_PATH = f"plot_{TRAINING_MODE}_mae.png"
PLOT_LOSS_PATH = f"plot_{TRAINING_MODE}_loss.png"

print(f"--- üöÄ Starting Experiment: {TRAINING_MODE} ---")
print(f"All files will be read from and saved to the current directory.")

# --- Removed Google Drive copy logic ---
# The script will now generate caches locally if they don't exist.

# ======================================================
# 1. Load Full Metadata
# ======================================================
print("Loading metadata...")
try:
    Y_full = pd.read_csv(META_CSV_PATH, index_col="ecg_id")
except FileNotFoundError:
    print(f"‚ùå ERROR: Metadata file not found at {META_CSV_PATH}")
    print("Please make sure 'ptbxl_database.csv' is in this folder.")
    raise

N_RECORDS = len(Y_full)
print(f"‚úÖ Loaded metadata for {N_RECORDS} records.")

# ======================================================
# 2. Preprocessing: Image Cache Generation (from Pickle)
# ======================================================

if not os.path.exists(IMAGE_CACHE_PATH):
    print(f"üõ†Ô∏è {TRAINING_MODE} image cache not found. Generating from pickle file...")

    # Load the source pickle data
    try:
        with open(SOURCE_PICKLE_PATH, 'rb') as f:
            pickle_data = pickle.load(f)
    except FileNotFoundError:
        print(f"‚ùå ERROR: Local pickle file not found at {SOURCE_PICKLE_PATH}")
        raise

    print(f"‚úÖ Loaded pickle file with {len(pickle_data)} entries.")

    # Create a writeable memmap for the new cache
    image_cache = np.memmap(IMAGE_CACHE_PATH, dtype=DTYPE, mode='w+', shape=(N_RECORDS, *IMG_SIZE, 3))

    valid_filenames = set(pickle_data.keys())
    missing_count = 0

    print(f"Building {TRAINING_MODE} cache...")
    for i, (ecg_id, row) in enumerate(Y_full.iterrows()):
        if i % 1000 == 0:
             print(f"  ...processing record {i}/{N_RECORDS}")
        filename = row["filename_hr"]

        # Check if we have data for this row
        if pd.isna(filename) or filename not in valid_filenames:
            missing_count += 1
            # Store a blank image if no data
            image_cache[i] = np.zeros((*IMG_SIZE, 3), dtype=DTYPE)
            continue

        # Load images, squeeze (1, 224, 224) -> (224, 224)
        ecg_img = pickle_data[filename]['ecg'].squeeze().astype(DTYPE)
        vcg_img = pickle_data[filename]['vcg'].squeeze().astype(DTYPE)

        final_image = np.zeros((*IMG_SIZE, 3), dtype=DTYPE)

        # Assemble the 3-channel image based on the mode
        if TRAINING_MODE == 'ecg_only':
            final_image[:, :, 0] = ecg_img
            final_image[:, :, 1] = ecg_img
            final_image[:, :, 2] = ecg_img

        elif TRAINING_MODE == 'vcg_only':
            final_image[:, :, 0] = vcg_img
            final_image[:, :, 1] = vcg_img
            final_image[:, :, 2] = vcg_img

        elif TRAINING_MODE == 'combined':
            avg_img = (ecg_img + vcg_img) / 2.0
            final_image[:, :, 0] = ecg_img
            final_image[:, :, 1] = vcg_img
            final_image[:, :, 2] = avg_img

        image_cache[i] = final_image

    image_cache.flush()
    del image_cache # Close memmap
    print(f"‚úÖ {TRAINING_MODE} cache generated.")
    if missing_count > 0:
        print(f"‚ö†Ô∏è Warning: {missing_count} records from metadata had no matching entry in the pickle file.")
else:
    print(f"‚úÖ {TRAINING_MODE} image cache found. Skipping image generation.")


# ======================================================
# 3. Preprocessing: Calculate Normalization Stats
# ======================================================
if not os.path.exists(STATS_PATH):
    print(f"üõ†Ô∏è Global stats for {TRAINING_MODE} not found. Calculating...")
    # Open the cache we just built
    image_cache = np.memmap(IMAGE_CACHE_PATH, dtype=DTYPE, mode='r', shape=(N_RECORDS, *IMG_SIZE, 3))

    pixel_sum = np.zeros(3, dtype=np.float64)
    pixel_sq_sum = np.zeros(3, dtype=np.float64)
    num_pixels = N_RECORDS * IMG_SIZE[0] * IMG_SIZE[1]

    print("Calculating Stats...")
    # Iterate in chunks to save RAM
    for i in range(0, N_RECORDS, 256):
        if i % 1024 == 0: # Print progress less often
            print(f"  ...stats chunk {i}/{N_RECORDS}")
        chunk = image_cache[i:i+256].astype(np.float64)
        pixel_sum += np.sum(chunk, axis=(0, 1, 2))
        pixel_sq_sum += np.sum(chunk**2, axis=(0, 1, 2))

    channel_mean = pixel_sum / num_pixels
    channel_std = np.sqrt(pixel_sq_sum / num_pixels - channel_mean**2)

    np.savez(STATS_PATH, mean=channel_mean, std=channel_std)
    print(f"‚úÖ Stats calculated and saved for {TRAINING_MODE}.")
    del image_cache # Close memmap
else:
    stats = np.load(STATS_PATH)
    channel_mean = stats['mean']
    channel_std = stats['std']
    print(f"‚úÖ Per-channel stats for {TRAINING_MODE} loaded.")

print(f"Channel Mean: {channel_mean}")
print(f"Channel Std: {channel_std}")

# ======================================================
# 4. Data Generator (with In-Memory Caching)
# ======================================================
class CustomDataGenerator(keras.utils.Sequence):
    def __init__(self, image_cache_path, labels, indices, batch_size, mean, std):
        self.indices = indices
        self.batch_size = batch_size
        self.mean = mean
        self.std = std
        self.labels = labels
        self.image_cache_path = image_cache_path

        # Preload data for this split into RAM
        print(f"üß† Preloading {len(indices)} samples for this generator...")
        # Open the full cache
        full_image_data_memmap = np.memmap(self.image_cache_path, dtype=np.float16, mode='r', shape=(N_RECORDS, *IMG_SIZE, 3))
        # Select only the indices for this split (train/val/test) and load into RAM
        self.X_cached = full_image_data_memmap[self.indices]
        self.y_cached = self.labels[self.indices]
        del full_image_data_memmap # Close memmap
        print("‚úÖ Preloading complete.")

    def __len__(self):
        return int(np.ceil(len(self.indices) / self.batch_size))

    def __getitem__(self, index):
        start = index * self.batch_size
        end = min((index + 1) * self.batch_size, len(self.indices))

        # Get preloaded batch
        X_batch = self.X_cached[start:end]
        y_batch = self.y_cached[start:end]

        # Apply normalization
        X_batch = (X_batch - self.mean) / (self.std + 1e-7)
        return X_batch, y_batch

# ======================================================
# 5. Model Building & Training (WITH AUGMENTATION)
# ======================================================

# --- Load metadata and define splits ---
# Y_full is already loaded from Section 1
y_labels = Y_full.age.values.astype(DTYPE)

# Get indices based on the full, unfiltered metadata
train_idx = np.where((Y_full.strat_fold != TEST_FOLD) & (Y_full.strat_fold != VAL_FOLD))[0]
val_idx = np.where(Y_full.strat_fold == VAL_FOLD)[0]
test_idx = np.where(Y_full.strat_fold == TEST_FOLD)[0]

# --- Instantiate generators ---
print("\n--- Initializing Data Generators ---")
train_generator = CustomDataGenerator(IMAGE_CACHE_PATH, y_labels, train_idx, BATCH_SIZE, channel_mean, channel_std)
val_generator = CustomDataGenerator(IMAGE_CACHE_PATH, y_labels, val_idx, BATCH_SIZE, channel_mean, channel_std)

# --- Build the model with a new head ---
print("\nüõ†Ô∏è Building model with augmentation layers...")

# Define the base ResNet50 model
base_model = keras.applications.ResNet50(
    weights="imagenet",
    include_top=False,
    input_shape=(224, 224, 3)
)

# --- NEW AUGMENTATION WORKFLOW ---

# 1. Create a new, standalone Input layer
inputs = keras.Input(shape=(224, 224, 3))

# 2. Add augmentation layers. These only run during training.
x = keras.layers.RandomBrightness(factor=0.08)(inputs)
x = keras.layers.RandomContrast(factor=0.08)(x)
x = keras.layers.RandomZoom(
    height_factor=(-0.2, 0.2),
    width_factor=(-0.2, 0.2),
    fill_mode='constant',
    fill_value=0.0
)(x)

# 3. Pass the augmented images to the base model
x = base_model(x)

# 4. Add your original model head
x = keras.layers.GlobalAveragePooling2D()(x)
x = keras.layers.Dense(512, activation="relu")(x)
x = keras.layers.Dropout(0.5)(x)
outputs = keras.layers.Dense(1, name="age_prediction")(x)

# 5. Create the final model
model = keras.Model(inputs=inputs, outputs=outputs)
print("‚úÖ Model built.")


# --- PHASE 1: Train the Head Only ---
print("\n--- PHASE 1: Training the model head ---")
base_model.trainable = False # Freeze the base

model.compile(
    optimizer=keras.optimizers.Adam(learning_rate=1e-3),
    loss="mean_squared_error",
    metrics=["mean_absolute_error"]
)

history_head = model.fit(
    train_generator,
    validation_data=val_generator,
    epochs=15,
    callbacks=[
        keras.callbacks.EarlyStopping(monitor="val_mean_absolute_error", patience=5, restore_best_weights=True)
    ]
)
print("‚úÖ Head training complete.")

# --- PHASE 2: Unfreeze and Fine-Tune Everything ---
print("\n--- PHASE 2: Fine-tuning the entire model ---")
base_model.trainable = True # Unfreeze the base

model.compile(
    optimizer=keras.optimizers.Adam(learning_rate=1e-5), # CRITICAL: Use a very low LR
    loss="mean_squared_error",
    metrics=["mean_absolute_error"]
)

history_fine_tune = model.fit(
    train_generator,
    validation_data=val_generator,
    epochs=50, # Train for more epochs
    callbacks=[
        keras.callbacks.ModelCheckpoint(BEST_MODEL_PATH, save_best_only=True, monitor="val_mean_absolute_error", mode="min"),
        keras.callbacks.EarlyStopping(monitor="val_mean_absolute_error", patience=10, mode="min")
    ]
)
print("‚úÖ Fine-tuning complete.")


# ======================================================
# 6. Final Evaluation
# ======================================================
print("\nüß™ Evaluating the best fine-tuned model on the test set...")
# Load the best weights saved by ModelCheckpoint
model.load_weights(BEST_MODEL_PATH)

# Create the test generator
test_generator = CustomDataGenerator(IMAGE_CACHE_PATH, y_labels, test_idx, BATCH_SIZE, channel_mean, channel_std)
test_loss, test_mae = model.evaluate(test_generator)

print("\n--- Evaluation Complete ---")
print(f"Mode: {TRAINING_MODE}")
print(f"üìâ Final Test Loss: {test_loss:.4f}")
print(f"üèÜ Final Test MAE: {test_mae:.4f}")


# ======================================================
# 7. Plot and Save Training History
# ======================================================
print("\nüìä Generating and saving training plots...")

try:
    # Combine history from the two training phases
    history = {}
    history['mean_absolute_error'] = history_head.history['mean_absolute_error'] + history_fine_tune.history['mean_absolute_error']
    history['val_mean_absolute_error'] = history_head.history['val_mean_absolute_error'] + history_fine_tune.history['val_mean_absolute_error']
    history['loss'] = history_head.history['loss'] + history_fine_tune.history['loss']
    history['val_loss'] = history_head.history['val_loss'] + history_fine_tune.history['val_loss']

    # --- Plot MAE ---
    plt.figure(figsize=(10, 6))
    plt.plot(history['mean_absolute_error'], label='Training MAE')
    plt.plot(history['val_mean_absolute_error'], label='Validation MAE')
    plt.axvline(x=len(history_head.history['loss'])-1, color='gray', linestyle='--', label='Start Fine-Tuning')
    plt.title(f'Training and Validation MAE ({TRAINING_MODE})')
    plt.xlabel('Epoch')
    plt.ylabel('Mean Absolute Error (Age)')
    plt.legend()
    plt.grid(True)
    plt.savefig(PLOT_MAE_PATH)
    print(f"‚úÖ MAE plot saved to {PLOT_MAE_PATH}")
    plt.close()

    # --- Plot Loss ---
    plt.figure(figsize=(10, 6))
    plt.plot(history['loss'], label='Training Loss')
    plt.plot(history['val_loss'], label='Validation Loss')
    plt.axvline(x=len(history_head.history['loss'])-1, color='gray', linestyle='--', label='Start Fine-Tuning')
    plt.title(f'Training and Validation Loss ({TRAINING_MODE})')
    plt.xlabel('Epoch')
    plt.ylabel('Loss (MSE)')
    plt.legend()
    plt.grid(True)
    plt.savefig(PLOT_LOSS_PATH)
    print(f"‚úÖ Loss plot saved to {PLOT_LOSS_PATH}")
    plt.close()

except Exception as e:
    print(f"‚ö†Ô∏è Error saving plots: {e}")

print("\n--- üéâ Experiment Finished ---")