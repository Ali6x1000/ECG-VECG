# -*- coding: utf-8 -*-
"""Train_ResNet_Age_Regression.ipynb

Automatically generated by Colab.
"""

import os
import pickle
import gc
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torchvision import models
from tqdm.auto import tqdm
from google.colab import drive

# --- 1. Setup & Constants ---

if not os.path.exists('/content/drive'):
    drive.mount('/content/drive')

VAL_FOLD = 9
TEST_FOLD = 10
IMG_SIZE = (224, 224)
BATCH_SIZE = 32
# We use float32 for training, but uint8 for storage to save RAM/Disk
TRAIN_DTYPE = np.float32 
STORAGE_DTYPE = np.uint8

NUM_EPOCHS = 50
LEARNING_RATE = 1e-4
PATIENCE = 7 
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

DRIVE_BASE_PATH = "/content/drive/MyDrive/CNN"
META_CSV_PATH = os.path.join(DRIVE_BASE_PATH, "ptbxl_database.csv")

# Pickle Paths
PATH_BOTH = os.path.join(DRIVE_BASE_PATH, "combined_ecg_vcg.pkl")
PATH_ECG_ONLY = os.path.join(DRIVE_BASE_PATH, "ecg_only.pkl")
PATH_VCG_ONLY = os.path.join(DRIVE_BASE_PATH, "vcg_only.pkl")

# Checkpoint Directory
CHECKPOINT_DIR = os.path.join(DRIVE_BASE_PATH, "Best")
os.makedirs(CHECKPOINT_DIR, exist_ok=True)

LOCAL_CACHE_DIR = "/content/cache"
os.makedirs(LOCAL_CACHE_DIR, exist_ok=True)

print(f"Running on device: {DEVICE}")

# --- 2. Data Processing Helpers ---

def load_metadata(csv_path):
    print(f"Loading metadata from {csv_path}...")
    try:
        df = pd.read_csv(csv_path, index_col="ecg_id")
    except FileNotFoundError:
        print(f"‚ùå ERROR: Metadata file not found at {csv_path}")
        raise

    # 1. Clean Data
    df = df.dropna(subset=['filename_hr'])
    df = df.dropna(subset=['age'])
    df['age'] = pd.to_numeric(df['age'], errors='coerce')
    df = df.dropna(subset=['age'])
    
    # --- Filter Age 18 to 85 ---
    pre_filter_len = len(df)
    df = df[(df['age'] >= 18) & (df['age'] <= 85)]
    print(f"Filtered ages 18-85: Dropped {pre_filter_len - len(df)} records.")

    # 2. Add Index for Memmap (Critical: Must be continuous 0 to N-1)
    df = df.reset_index(drop=False) # Keep ecg_id as column, reset index to 0..N
    df['memmap_idx'] = df.index.values # Explicitly assign the new sequential index
    
    # 3. SAFETY CHECK (Fix for Issue #1)
    assert df['memmap_idx'].min() == 0, "Index must start at 0"
    assert df['memmap_idx'].max() == len(df) - 1, "Index must be continuous"
    assert df['memmap_idx'].is_unique, "Indices must be unique"
    
    print(f"‚úÖ Metadata loaded. {len(df)} records ready.")
    return df

# --- 3. Memmap Generation (Optimized for uint8) ---

def prepare_cache(dataset_name, pickle_path, df_meta):
    # Changed filename to indicate uint8 storage
    cache_path = os.path.join(LOCAL_CACHE_DIR, f"{dataset_name}_age_uint8.dat")
    n_records = len(df_meta)
    
    # --- Image Cache Generation ---
    if not os.path.exists(cache_path):
        print(f"üõ†Ô∏è {dataset_name} cache not found. Generating from pickle...")
        
        with open(pickle_path, 'rb') as f:
            pickle_data = pickle.load(f)
        
        # Create memmap with uint8 directly (Fix for Issue #2)
        fp = np.memmap(cache_path, dtype=STORAGE_DTYPE, mode='w+', shape=(n_records, *IMG_SIZE, 3))
        valid_filenames = set(pickle_data.keys())
        
        for idx, row in tqdm(df_meta.iterrows(), total=n_records, desc="Building Cache"):
            filename = row["filename_hr"]
            memmap_idx = int(row["memmap_idx"])
            
            if filename in valid_filenames:
                img = pickle_data[filename]
                img = img.squeeze()
                
                # --- Normalize & Cast to uint8 BEFORE saving ---
                # This ensures what is on disk is exactly what we load
                if img.max() <= 1.5: 
                    # Scale from [min, max] to [0, 255]
                    img = (img - img.min()) / (img.max() - img.min() + 1e-6) 
                    img = img * 255.0 
                
                # Cast to uint8
                img_uint8 = img.astype(np.uint8)

                # Stack Channels
                if img_uint8.ndim == 2:
                    img_3ch = np.stack([img_uint8, img_uint8, img_uint8], axis=-1)
                elif img_uint8.ndim == 3 and img_uint8.shape[-1] == 3:
                    img_3ch = img_uint8
                else:
                    img_3ch = np.zeros((*IMG_SIZE, 3), dtype=np.uint8)

                fp[memmap_idx] = img_3ch
            else:
                fp[memmap_idx] = np.zeros((*IMG_SIZE, 3), dtype=np.uint8)
                
        fp.flush()
        del fp, pickle_data
        gc.collect()
    else:
        print(f"‚úÖ {dataset_name} cache found (uint8 optimized).")
        
    return cache_path, n_records

# --- 4. Dataset Class ---

class RAMDataset(Dataset):
    def __init__(self, df, memmap_path, total_records):
        self.indices = df['memmap_idx'].values
        self.labels = df['age'].values.astype(np.float32)
        
        # ImageNet Statistics
        self.imagenet_mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)
        self.imagenet_std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)
        
        print(f"‚ö° Loading {len(self.indices)} images into RAM...")
        
        # Open as uint8 (Matches file format now)
        fp = np.memmap(memmap_path, dtype=STORAGE_DTYPE, mode='r', shape=(total_records, *IMG_SIZE, 3))
        
        # Load directly to RAM
        self.data = fp[self.indices] # already uint8
        del fp 
        
    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        # 1. Get uint8 image
        img = self.data[idx]
        
        # 2. Convert to float [0, 1] for model
        img = img.astype(TRAIN_DTYPE) / 255.0
        
        # 3. To Tensor & Normalize
        tensor = torch.from_numpy(img).permute(2, 0, 1)
        tensor = (tensor - self.imagenet_mean) / self.imagenet_std
        
        return tensor, torch.tensor(self.labels[idx], dtype=torch.float32)

# --- 5. Model Definition ---

def get_age_model():
    model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)
    num_ftrs = model.fc.in_features
    # Linear Layer only (No ReLU)
    model.fc = nn.Linear(num_ftrs, 1) 
    return model

# --- 6. Training & Evaluation Engine ---

def train_and_evaluate(dataset_name, pickle_path, df_meta):
    print(f"\n{'='*40}")
    print(f"STARTING AGE REGRESSION: {dataset_name}")
    print(f"{'='*40}")
    
    torch.backends.cudnn.benchmark = True 
    
    memmap_path, total_records = prepare_cache(dataset_name, pickle_path, df_meta)
    
    # 1. Create Datasets & Loaders
    train_df = df_meta[df_meta['strat_fold'] < VAL_FOLD]
    val_df = df_meta[df_meta['strat_fold'] == VAL_FOLD]
    test_df = df_meta[df_meta['strat_fold'] == TEST_FOLD]
    
    train_dataset = RAMDataset(train_df, memmap_path, total_records)
    val_dataset = RAMDataset(val_df, memmap_path, total_records)
    test_dataset = RAMDataset(test_df, memmap_path, total_records)
    
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, 
                              num_workers=os.cpu_count(), pin_memory=True, persistent_workers=True)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, 
                            num_workers=os.cpu_count(), pin_memory=True, persistent_workers=True)
    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False,
                             num_workers=os.cpu_count(), pin_memory=True) 
    
    # 2. Setup Model
    model = get_age_model().to(DEVICE)
    criterion = nn.MSELoss()
    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)
    
    # AMP Scaler
    scaler = torch.amp.GradScaler('cuda') 
    
    # Scheduler
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.5, patience=3
    )
    
    best_val_mae = float('inf') 
    save_path = os.path.join(CHECKPOINT_DIR, f"resnet50_age_{dataset_name}_best.pth")
    patience_counter = 0
    
    # 3. Training Loop
    for epoch in range(NUM_EPOCHS):
        model.train()
        running_mae = 0.0
        
        pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}", leave=False)
        for images, labels in pbar:
            images = images.to(DEVICE, non_blocking=True)
            labels = labels.view(-1, 1).to(DEVICE, non_blocking=True)
            
            optimizer.zero_grad(set_to_none=True)
            
            with torch.amp.autocast('cuda'):
                outputs = model(images)
                loss = criterion(outputs, labels)
            
            scaler.scale(loss).backward()
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            scaler.step(optimizer)
            scaler.update()
            
            mae = torch.abs(outputs.float() - labels.float()).sum()
            running_mae += mae.item()
            pbar.set_postfix({'loss': loss.item()})
            
        epoch_mae = running_mae / len(train_dataset)
        
        # Validation Phase
        model.eval()
        val_mae = 0.0
        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(DEVICE, non_blocking=True)
                labels = labels.view(-1, 1).to(DEVICE, non_blocking=True)
                outputs = model(images)
                val_mae += torch.abs(outputs - labels).sum().item()
                
        epoch_val_mae = val_mae / len(val_dataset)
        
        scheduler.step(epoch_val_mae)
        print(f"Epoch {epoch+1}: Train MAE: {epoch_mae:.2f} | Val MAE: {epoch_val_mae:.2f}")
        
        # Save Best Model
        if epoch_val_mae < best_val_mae:
            best_val_mae = epoch_val_mae
            patience_counter = 0 
            torch.save(model.state_dict(), save_path)
            print("  >>> Saved Best Model")
        else:
            patience_counter += 1
            if patience_counter >= PATIENCE:
                print("üõë Early stopping triggered.")
                break
    
    # 4. FINAL TEST EVALUATION
    print(f"\n{'='*40}")
    print(f"üîé RUNNING FINAL TEST ON: {dataset_name}")
    print(f"{'='*40}")
    
    if os.path.exists(save_path):
        print(f"Loading best weights from: {save_path}")
        model.load_state_dict(torch.load(save_path))
    else:
        print("‚ö†Ô∏è Warning: Best model file not found. Using last model state.")
        
    model.eval()
    test_mse_sum = 0.0
    test_mae_sum = 0.0
    
    with torch.no_grad():
        for images, labels in tqdm(test_loader, desc="Testing"):
            images = images.to(DEVICE, non_blocking=True)
            labels = labels.view(-1, 1).to(DEVICE, non_blocking=True)
            
            outputs = model(images)
            
            batch_mse = nn.MSELoss(reduction='sum')(outputs, labels)
            batch_mae = torch.abs(outputs - labels).sum()
            
            test_mse_sum += batch_mse.item()
            test_mae_sum += batch_mae.item()
            
    final_test_mse = test_mse_sum / len(test_dataset)
    final_test_mae = test_mae_sum / len(test_dataset)
    final_test_rmse = np.sqrt(final_test_mse)
    
    print(f"\nüèÜ FINAL TEST RESULTS for {dataset_name}:")
    print(f"   MSE : {final_test_mse:.4f}")
    print(f"   RMSE: {final_test_rmse:.4f}")
    print(f"   MAE : {final_test_mae:.4f}")
    print(f"{'='*40}\n")
            
    del model, optimizer, train_loader, val_loader, test_loader
    gc.collect()
    torch.cuda.empty_cache()

# --- 7. Main Execution ---

if __name__ == "__main__":
    try:
        df_meta = load_metadata(META_CSV_PATH)
        
        print(f"üìä Age Statistics (After Filter):")
        print(f"   Range: {df_meta['age'].min():.1f} - {df_meta['age'].max():.1f}")
        
        tasks = [
            ("Both", PATH_BOTH),
            ("ECG_Only", PATH_ECG_ONLY),
            ("VCG_Only", PATH_VCG_ONLY)
        ]
        for name, path in tasks:
            train_and_evaluate(name, path, df_meta)
    except Exception as e:
        print(f"Script failed: {e}")
        import traceback
        traceback.print_exc()